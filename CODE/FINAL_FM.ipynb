{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T21:38:59.763730Z",
     "start_time": "2024-07-20T21:38:59.118207Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from stats import *\n",
    "from utils import *\n",
    "import visual as vi\n",
    "from Inlier_Thresholder import Inlier_Thresholder\n",
    "import scipy.io as spi\n",
    "from time import time\n",
    "import os\n",
    "import seaborn as sns\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T21:39:00.045126Z",
     "start_time": "2024-07-20T21:38:59.766181Z"
    }
   },
   "outputs": [],
   "source": [
    "directory = 'C:/Users/carlo/IACV PROJECT/adelFM'\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Filter out only the .mat files\n",
    "names = [file for file in files if file.endswith('.mat')]\n",
    "\n",
    "# Load each .mat file\n",
    "mat_data = {}\n",
    "for file in names:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    mat_data[file] = spi.loadmat(file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) INLIER THRESHOLDS ESTIMATION\n",
    "\n",
    "The plots of the silhouette scores are done only if some outliers are present, otherwise a simple plot with the threshold is displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a) SED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T21:39:01.202703Z",
     "start_time": "2024-07-20T21:39:00.046278Z"
    }
   },
   "outputs": [],
   "source": [
    "# input parameter, how many files to analyse\n",
    "number_of_data_to_analyse=len(names)\n",
    "\n",
    "thresholds = [[] for i in range(number_of_data_to_analyse)]\n",
    "\n",
    "for i in range(number_of_data_to_analyse):\n",
    "    print(i)\n",
    "    data = mat_data[names[i]]\n",
    "    res, n_inl, n_outl = compute_inliers_residual_curve(data, type='FM', return_inl_outl=True, verbose=False, metric=\"sed\")\n",
    "\n",
    "    for j in range(len(res)):\n",
    "        anomaly_detector=Inlier_Thresholder(res[j], n_inl[j], n_outl[j],  alphas={\"Median AD\": 4 , \"Variance based\": 5 ,\"Rosseeuw SN\":4,\"Rosseeuw QN\":4 })\n",
    "\n",
    "        labels, threshold=anomaly_detector.compute_inlier_threshold(\"Variance based\") #use_best_method()  # use best method among the statistical ones, best according to silhouette\n",
    "        #print(\"eccolo: \", threshold)\n",
    "        thresholds[i].append(threshold)\n",
    "        \n",
    "        if len(np.unique(labels))!=1:\n",
    "            silhouette_scr, silhouette_avg, values = silhouette_score_and_average(res[j], labels)\n",
    "            plot_silhouette(silhouette_scr, labels, silhouette_avg, values, -thresholds[i][j])\n",
    "            \n",
    "        else:\n",
    "            plt.figure()\n",
    "            plt.scatter(np.arange(len(res[j])),-np.sort(res[j]), marker='.', s=30, lw=0, alpha=0.7)\n",
    "            plt.axhline(y=-thresholds[i][j], color=\"purple\" , linestyle=\"--\")\n",
    "            plt.show()\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) SAMPSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameter, how many files to analyse\n",
    "number_of_data_to_analyse=len(names)\n",
    "\n",
    "thresholds_1 = [[] for i in range(number_of_data_to_analyse)]\n",
    "\n",
    "for i in range(number_of_data_to_analyse):\n",
    "    print(i)\n",
    "    data = mat_data[names[i]]\n",
    "    res, n_inl, n_outl = compute_inliers_residual_curve(data, type='FM', return_inl_outl=True, verbose=False, metric=\"sampson\")\n",
    "\n",
    "    for j in range(len(res)):\n",
    "        anomaly_detector=Inlier_Thresholder(res[j], n_inl[j], n_outl[j],  alphas={\"Median AD\": 4 , \"Variance based\": 5 ,\"Rosseeuw SN\":4,\"Rosseeuw QN\":4 })\n",
    "\n",
    "        labels, threshold=anomaly_detector.compute_inlier_threshold(\"Variance based\") #use_best_method()  # use best method among the statistical ones, best according to silhouette\n",
    "        #print(\"eccolo: \", threshold)\n",
    "        thresholds_1[i].append(threshold)\n",
    "        \n",
    "        if len(np.unique(labels))!=1:\n",
    "            silhouette_scr, silhouette_avg, values = silhouette_score_and_average(res[j], labels)\n",
    "            plot_silhouette(silhouette_scr, labels, silhouette_avg, values, -thresholds_1[i][j])\n",
    "            \n",
    "        else:\n",
    "            plt.figure()\n",
    "            plt.scatter(np.arange(len(res[j])),-np.sort(res[j]), marker='.', s=30, lw=0, alpha=0.7)\n",
    "            plt.axhline(y=-thresholds_1[i][j], color=\"purple\" , linestyle=\"--\")\n",
    "            plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) ENSEMBLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ens_labels_1=[]\n",
    "for i in range(len(thresholds_1)):\n",
    "    print(i)\n",
    "    data=mat_data[names[i]]\n",
    "    ens = build_ensemble_mask(data, plot=False, verbose=False, threshold=thresholds[i])\n",
    "    ens1 = build_ensemble_mask(data, plot=False, verbose=False, threshold=thresholds_1[i])\n",
    "    \n",
    "    im1=data[\"img1\"]\n",
    "    im2=data[\"img2\"]\n",
    "    \n",
    "    outliers, models = vi.group_models(data)[\"outliers\"], vi.group_models(data)[\"models\"]\n",
    "\n",
    "    points=extract_points(models,data)\n",
    "\n",
    "    tot_src=points[1]\n",
    "    tot_dst=points[2]\n",
    "\n",
    "    points=points[0]\n",
    "    \n",
    "    image_lab=[]\n",
    "    \n",
    "    for j in range(len(models)):\n",
    "        \n",
    "    \n",
    "        ensemble=np.where(ens[j]+ens1[j]<2,0,1)\n",
    "        image_lab.append(ensemble)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        src = points[\"src_points\"][j][np.where(ensemble==0)]\n",
    "        dst = points[\"dst_points\"][j][np.where(ensemble==0)]\n",
    "        plt.figure()\n",
    "        draw_matches(im1,im2,src,dst)\n",
    "        plt.show()\n",
    "    ens_labels_1.append(image_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) INFLUENCE FUNCTION SCORE OF THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T21:39:16.394856Z",
     "start_time": "2024-07-20T21:39:16.394790Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## outlier scores for a single model of a single image\n",
    "def outlier_score(src , dst):\n",
    "    \n",
    "    ## initial quantities\n",
    "    N=src.shape[0]\n",
    "    \n",
    "    ## fit the baseline\n",
    "    FM,baseline_mask=cv2.findFundamentalMat(src, dst, method=cv2.FM_8POINT)#verify_pygcransac_H(src,dst,img1,img2, threshold, verbose=False)\n",
    "    \n",
    "    ## initialize the return quantity\n",
    "    outlier_scores_FM=np.zeros(N)\n",
    "    \n",
    "    \n",
    "    ## loop through all the points\n",
    "    for i in range(N):\n",
    "        src_i = np.delete(src, i, axis=0)\n",
    "        dst_i = np.delete(dst, i, axis=0)\n",
    "        \n",
    "        ## fit the model\n",
    "        FM_i,mask_i=cv2.findFundamentalMat(src_i, dst_i, method=cv2.FM_8POINT)#verify_pygcransac_H(src_i,dst_i,img1,img2, threshold, verbose=False)\n",
    "\n",
    "        ## outlier score\n",
    "        outlier_scores_FM[i]=sum(sum((FM-FM_i)**2))\n",
    "        \n",
    "    ## return\n",
    "    return outlier_scores_FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T21:39:16.530616Z",
     "start_time": "2024-07-20T21:39:16.396546Z"
    }
   },
   "outputs": [],
   "source": [
    "outlier_scores_FM=[]\n",
    "\n",
    "for i in range(number_of_data_to_analyse):\n",
    "    data=mat_data[names[i]]\n",
    "\n",
    "    img1, img2 = data[\"img1\"], data[\"img2\"]\n",
    "\n",
    "    outliers, models = vi.group_models(data)[\"outliers\"], vi.group_models(data)[\"models\"]\n",
    "\n",
    "    points=extract_points(models,data)\n",
    "\n",
    "    tot_src=points[1]\n",
    "    tot_dst=points[2]\n",
    "\n",
    "    points=points[0]\n",
    "\n",
    "    out_score_img_i_FM=[]\n",
    "\n",
    "    for m in range(len(models)):\n",
    "\n",
    "\n",
    "        src = points[\"src_points\"][m]\n",
    "        dst = points[\"dst_points\"][m]\n",
    "\n",
    "\n",
    "        out_score_img_i_FM.append(outlier_score(src,dst))\n",
    "\n",
    "    outlier_scores_FM.append(out_score_img_i_FM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T21:39:16.549617Z",
     "start_time": "2024-07-20T21:39:16.538483Z"
    }
   },
   "outputs": [],
   "source": [
    "ratios=[]\n",
    "\n",
    "for j in range(number_of_data_to_analyse):\n",
    "    image_ratio=[]\n",
    "\n",
    "    for i in range(len(outlier_scores_FM[j])):\n",
    "         print(f\"img{j} model{i}\")\n",
    "         print(f\"inliers avarage score ({sum(ens_labels_1[j][i])}): \",np.mean(outlier_scores_FM[j][i][np.where(ens_labels_1[j][i]==1)]),f\" Outliers avarage score ({len(ens_labels_1[j][i])-(sum(ens_labels_1[j][i]))}): \" ,\n",
    "               np.mean(outlier_scores_FM[j][i][np.where(ens_labels_1[j][i]==0)]), f\"Factor of the two: \", np.mean(outlier_scores_FM[j][i][np.where(ens_labels_1[j][i]==0)])/np.mean(outlier_scores_FM[j][i][np.where(ens_labels_1[j][i]==1)]))\n",
    "        \n",
    "         image_ratio.append(np.mean(outlier_scores_FM[j][i][np.where(ens_labels_1[j][i]==0)])/np.mean(outlier_scores_FM[j][i][np.where(ens_labels_1[j][i]==1)]))\n",
    "        \n",
    "    ratios.append(image_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Correct only the immages for which it is worth it (high influence function ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_corrections=[]\n",
    "\n",
    "final_labels=[]\n",
    "print(\"These are the images and models for which the correction is not worth it: \\n\")\n",
    "for i in range(number_of_data_to_analyse):\n",
    "    image_labels=[]\n",
    "    \n",
    "    for j in range(len(outlier_scores_FM[i])):\n",
    "        model_labels=[]\n",
    "        if ratios[i][j]<30:\n",
    "            print(f\"Image {i} Model {j}\")\n",
    "            useless_corrections.append((i,j))\n",
    "            \n",
    "            model_labels=np.ones(len(ens_labels_1[i][j])).astype(int)\n",
    "            \n",
    "        else:\n",
    "            model_labels=ens_labels_1[i][j]\n",
    "            \n",
    "        image_labels.append(model_labels)\n",
    "        \n",
    "    \n",
    "    final_labels.append(image_labels)     \n",
    "            \n",
    "            #ens_labels_1[i][j]=np.ones(len(ens_labels_1[i][j])).astype(int)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(thresholds)):\n",
    "    print(i)\n",
    "    data=mat_data[names[i]]\n",
    "\n",
    "    \n",
    "    im1=data[\"img1\"]\n",
    "    im2=data[\"img2\"]\n",
    "    \n",
    "    outliers, models = vi.group_models(data)[\"outliers\"], vi.group_models(data)[\"models\"]\n",
    "\n",
    "    points=extract_points(models,data)\n",
    "\n",
    "    tot_src=points[1]\n",
    "    tot_dst=points[2]\n",
    "\n",
    "    points=points[0]\n",
    "    \n",
    "    image_lab=[]\n",
    "    \n",
    "    \n",
    "    for j in range(len(models)):\n",
    "        \n",
    "        if (i,j) not in useless_corrections:\n",
    "            lab=final_labels[i][j]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "            src = points[\"src_points\"][j][np.where(lab==0)]\n",
    "            dst = points[\"dst_points\"][j][np.where(lab==0)]\n",
    "            plt.figure()\n",
    "            draw_matches(im1,im2,src,dst)\n",
    "            plt.show()\n",
    "            \n",
    "        else:\n",
    "            src = np.array([])#points[\"src_points\"][j]\n",
    "            dst = np.array([])# points[\"dst_points\"][j]\n",
    "            plt.figure()\n",
    "            draw_matches(im1,im2,src,dst)\n",
    "            plt.show()\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-20T21:39:31.365758Z",
     "start_time": "2024-07-20T21:39:16.553520Z"
    }
   },
   "source": [
    "influence_labels=[]\n",
    "\n",
    "for j in range(number_of_data_to_analyse):\n",
    "    \n",
    "    image_labels=[]\n",
    "\n",
    "    for i in range(len(outlier_scores_FM[j])):\n",
    "        #m=(outlier_scores_FM[j][i]>=np.median(outlier_scores_FM[j][i])+2*np.std(outlier_scores_FM[j][i]))\n",
    "        anomaly_detector=Inlier_Thresholder(outlier_scores_FM[j][i],  alphas={\"Median AD\": 5 , \"Variance based\": 1.2 ,\"Rosseeuw SN\":1.5,\"Rosseeuw QN\":5 })\n",
    "        \n",
    "        \n",
    "        labels, threshold=anomaly_detector.compute_inlier_threshold(\"Variance based\")  # use best method among the statistical ones, best according to silhouette\n",
    "\n",
    "        #labels 1 outlier 0 inlier\n",
    "        m=abs(labels-1)\n",
    "        \n",
    "        image_labels.append(m)\n",
    "        if len(np.unique(m))==2: \n",
    "            silhouette_scr, silhouette_avg, values = silhouette_score_and_average(outlier_scores_FM[j][i], m)\n",
    "        \n",
    "       # m=abs(m.astype(int)-1)\n",
    "            plot_silhouette(silhouette_scr, m, silhouette_avg=silhouette_avg, values=values, threhsold=-threshold)\n",
    "\n",
    "        data=mat_data[names[j]]\n",
    "        im1=data[\"img1\"]\n",
    "        im2=data[\"img2\"]\n",
    "\n",
    "        outliers, models = vi.group_models(data)[\"outliers\"], vi.group_models(data)[\"models\"]\n",
    "\n",
    "        points=extract_points(models,data)\n",
    "\n",
    "        tot_src=points[1]\n",
    "        tot_dst=points[2]\n",
    "\n",
    "        points=points[0]\n",
    "\n",
    "        src = points[\"src_points\"][i][np.where(m==0)]\n",
    "        dst = points[\"dst_points\"][i][np.where(m==0)]\n",
    "        plt.figure()\n",
    "        draw_matches(im1,im2,src,dst)\n",
    "        plt.show()\n",
    "        \n",
    "    influence_labels.append(image_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ens_labels_influence_ensemble=[]\n",
    "for i in range(len(thresholds)):\n",
    "    print(i)\n",
    "    data=mat_data[names[i]]\n",
    "\n",
    "    \n",
    "    im1=data[\"img1\"]\n",
    "    im2=data[\"img2\"]\n",
    "    \n",
    "    outliers, models = vi.group_models(data)[\"outliers\"], vi.group_models(data)[\"models\"]\n",
    "\n",
    "    points=extract_points(models,data)\n",
    "\n",
    "    tot_src=points[1]\n",
    "    tot_dst=points[2]\n",
    "\n",
    "    points=points[0]\n",
    "    \n",
    "    image_lab=[]\n",
    "    \n",
    "    \n",
    "    for j in range(len(models)):\n",
    "        \n",
    "    \n",
    "        ensemble=np.where(influence_labels[i][j]+ens_labels_1[i][j]<1,0,1)\n",
    "        image_lab.append(ensemble)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        src = points[\"src_points\"][j][np.where(ensemble==0)]\n",
    "        dst = points[\"dst_points\"][j][np.where(ensemble==0)]\n",
    "        plt.figure()\n",
    "        draw_matches(im1,im2,src,dst)\n",
    "        plt.show()\n",
    "        \n",
    "    ens_labels_influence_ensemble.append(image_lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
