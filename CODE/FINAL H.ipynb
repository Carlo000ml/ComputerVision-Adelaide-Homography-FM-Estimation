{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a44a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from stats import *\n",
    "from utils import *\n",
    "import visual as vi\n",
    "from Inlier_Thresholder import Inlier_Thresholder\n",
    "import scipy.io as spi\n",
    "from time import time\n",
    "import os\n",
    "import seaborn as sns\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc76ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'C:/Users/carlo/IACV PROJECT/adelH'\n",
    "\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory)\n",
    "\n",
    "# Filter out only the .mat files\n",
    "names = [file for file in files if file.endswith('.mat')]\n",
    "\n",
    "# Load each .mat file\n",
    "mat_data = {}\n",
    "for file in names:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    mat_data[file] = spi.loadmat(file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea26b45",
   "metadata": {},
   "source": [
    "# 1) LMEDS -> INLIER THRESHOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ae545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameter, how many files to analyse\n",
    "number_of_data_to_analyse=len(names)\n",
    "\n",
    "\n",
    "thresholds = [[] for i in range(number_of_data_to_analyse)]\n",
    "res_matrices=[]\n",
    "part_matrices=[]\n",
    "soft_matrices=[]\n",
    "\n",
    "H_lmeds=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(number_of_data_to_analyse):\n",
    "    data=mat_data[names[i]]\n",
    "    res_mat=build_residual_matrix(data,verbose=False)\n",
    "    \n",
    "    res_matrices.append(res_mat)\n",
    "    part_matrices.append(build_partition_matrix(res_mat))\n",
    "    \n",
    "    res=compute_inliers_residual_curve(data)\n",
    "    \n",
    "    data=mat_data[names[i]]\n",
    "    \n",
    "    img1, img2 = data[\"img1\"], data[\"img2\"]\n",
    "\n",
    "    outliers, models = vi.group_models(data)[\"outliers\"], vi.group_models(data)[\"models\"]\n",
    "    \n",
    "    points=extract_points(models,data)\n",
    "    \n",
    "    labels = points[3]\n",
    "    \n",
    "    points=points[0]\n",
    "    \n",
    "    H_models=[]\n",
    "    \n",
    "    for m in range(len(models)):\n",
    "        src = points[\"src_points\"][m]\n",
    "        dst = points[\"dst_points\"][m]\n",
    "        \n",
    "        H,_=verify_LMEDS_H(src,dst,verbose=False)\n",
    "        \n",
    "        H_models.append(H)\n",
    "        \n",
    "    H_lmeds.append(H_models)\n",
    "    #print(len(res))\n",
    "\n",
    "    for j in range(len(res)):\n",
    "        anomaly_detector=Inlier_Thresholder(res[j])\n",
    "\n",
    "        labels, threshold=anomaly_detector.use_best_method(verbose=False)  # use best method among the statistical ones, best according to silhouette\n",
    "        #print(\"eccolo: \", threshold)\n",
    "        thresholds[i].append(threshold)\n",
    "            \n",
    "    #print(res_mat.shape)\n",
    "    soft_matrices.append(soft_clustering_assignment(res_mat,thresholds[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf79fc",
   "metadata": {},
   "source": [
    "# 2.a) GC RANSAC WITH INLIER THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e10398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameter, how many files to analyse\n",
    "number_of_data_to_analyse= len(names)\n",
    "\n",
    "\n",
    "\n",
    "res_matrices=[]\n",
    "part_matrices=[]\n",
    "H_gc=[]\n",
    "\n",
    "residuals=[]\n",
    "soft_clus_assignment=[]\n",
    "gc_masks=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(number_of_data_to_analyse):\n",
    "    data=mat_data[names[i]]\n",
    "    \n",
    "    img1, img2 = data[\"img1\"], data[\"img2\"]\n",
    "\n",
    "    outliers, models = vi.group_models(data)[\"outliers\"], vi.group_models(data)[\"models\"]\n",
    "    \n",
    "    points=extract_points(models,data)\n",
    "    \n",
    "    points=points[0]\n",
    "    \n",
    "    H_models=[]\n",
    "    \n",
    "    H_res_models=[]\n",
    "    \n",
    "    gc_masks_i=[]\n",
    "    \n",
    "    for m in range(len(models)):\n",
    "        src = points[\"src_points\"][m]\n",
    "        dst = points[\"dst_points\"][m]\n",
    "        \n",
    "        H,mask_i=verify_pygcransac_H(src,dst,img1,img2,threshold=thresholds[i][m],verbose=False)\n",
    "        \n",
    "        H_models.append(H)\n",
    "        \n",
    "        H_residual= residual_H1_wrt_H2(src,H,H_lmeds[i][m])\n",
    "        \n",
    "        H_res_models.append(H_residual)\n",
    "        \n",
    "        gc_masks_i.append(mask_i)\n",
    "        \n",
    "    gc_masks.append(gc_masks_i)\n",
    "        \n",
    "    residuals.append(H_res_models)\n",
    "        \n",
    "    H_gc.append(H_models)\n",
    "    \n",
    "    \n",
    "    res=build_residual_matrix(data, plot=True, verbose=False, method=\"gc-ransac\" , threshold=thresholds[i])\n",
    "    part_mat=build_partition_matrix(res)\n",
    "    labl=data[\"label\"]\n",
    "    mod_lab=np.where(labl>0)\n",
    "    labl=labl[mod_lab].reshape(-1,1)\n",
    "    labl=labl/np.max(labl)\n",
    "    labl=np.abs(np.max(part_mat)-labl*np.max(part_mat))\n",
    "    \n",
    "    res_matrices.append(res)\n",
    "    part_matrices.append(part_mat)\n",
    "    plot_residual_matrix(part_mat,labl,title=\"a\")\n",
    "    \n",
    "    soft_clustering=soft_clustering_assignment(res,thresholds[i])\n",
    "    soft_clus_assignment.append(soft_clustering)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570d3b6",
   "metadata": {},
   "source": [
    "# 2.b) INFLUENCE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b085c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## outlier scores for a single model of a single image\n",
    "def outlier_score(src , dst):\n",
    "    \n",
    "    ## initial quantities\n",
    "    N=src.shape[0]\n",
    "    \n",
    "    ## fit the baseline\n",
    "    H,baseline_mask=cv2.findHomography(src, dst, method=0)#verify_pygcransac_H(src,dst,img1,img2, threshold, verbose=False)\n",
    "    \n",
    "    ## initialize the return quantity\n",
    "    outlier_scores_H=np.zeros(N)\n",
    "    \n",
    "    \n",
    "    ## loop through all the points\n",
    "    for i in range(N):\n",
    "        src_i = np.delete(src, i, axis=0)\n",
    "        dst_i = np.delete(dst, i, axis=0)\n",
    "        \n",
    "        ## fit the model\n",
    "        H_i,mask_i=cv2.findHomography(src_i, dst_i, method=0)#verify_pygcransac_H(src_i,dst_i,img1,img2, threshold, verbose=False)\n",
    "\n",
    "        ## outlier score\n",
    "        outlier_scores_H[i]=sum(sum((H-H_i)**2))\n",
    "        \n",
    "    ## return\n",
    "    return outlier_scores_H\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c600b829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameter, how many files to analyse\n",
    "number_of_data_to_analyse=len(names)\n",
    "\n",
    "outlier_scores_H=[]\n",
    "\n",
    "for i in range(number_of_data_to_analyse):\n",
    "    data=mat_data[names[i]]\n",
    "    \n",
    "    img1, img2 = data[\"img1\"], data[\"img2\"]\n",
    "\n",
    "    outliers, models = vi.group_models(data)[\"outliers\"], vi.group_models(data)[\"models\"]\n",
    "    \n",
    "    points=extract_points(models,data)\n",
    "    \n",
    "    tot_src=points[1]\n",
    "    tot_dst=points[2]\n",
    "    \n",
    "    points=points[0]\n",
    "    \n",
    "    out_score_img_i_H=[]    \n",
    "    \n",
    "    for m in range(len(models)):\n",
    "        \n",
    "        \n",
    "        src = points[\"src_points\"][m]\n",
    "        dst = points[\"dst_points\"][m]\n",
    "        \n",
    "\n",
    "        out_score_img_i_H.append(outlier_score(src,dst))    \n",
    "        \n",
    "    outlier_scores_H.append(out_score_img_i_H)  \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b1dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(number_of_data_to_analyse):\n",
    "    \n",
    "    for i in range(len(outlier_scores_H[j])):\n",
    "         print(f\"img{j} model{i}\")\n",
    "         print(f\"inliers avarage score ({sum(gc_masks[j][i])}): \",np.mean(outlier_scores_H[j][i][np.where(gc_masks[j][i]==1)]),f\" Outliers avarage score ({len(gc_masks[j][i])-(sum(gc_masks[j][i]))}): \" ,\n",
    "               np.mean(outlier_scores_H[j][i][np.where(gc_masks[j][i]==0)]), f\"Factor of the two: \", np.mean(outlier_scores_H[j][i][np.where(gc_masks[j][i]==0)])/np.mean(outlier_scores_H[j][i][np.where(gc_masks[j][i]==1)]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00564104",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(number_of_data_to_analyse):\n",
    "    \n",
    "    for i in range(len(outlier_scores_H[j])):\n",
    "        m=(outlier_scores_H[j][i]>=np.median(outlier_scores_H[j][i])+3.5*np.std(outlier_scores_H[j][i]))\n",
    "        m=abs(m.astype(int)-1)\n",
    "\n",
    "        data=mat_data[names[j]]\n",
    "        im1=data[\"img1\"]\n",
    "        im2=data[\"img2\"]\n",
    "\n",
    "        outliers, models = vi.group_models(data)[\"outliers\"], vi.group_models(data)[\"models\"]\n",
    "\n",
    "        points=extract_points(models,data)\n",
    "\n",
    "        tot_src=points[1]\n",
    "        tot_dst=points[2]\n",
    "\n",
    "        points=points[0]\n",
    "\n",
    "        src = points[\"src_points\"][i][np.where(m==0)]\n",
    "        dst = points[\"dst_points\"][i][np.where(m==0)]\n",
    "        plt.figure()\n",
    "        draw_matches(im1,im2,src,dst)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9833d71d",
   "metadata": {},
   "source": [
    "# 3) GC RANSAC ON INLIERS AND CORRECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0366c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameter, how many files to analyse\n",
    "number_of_data_to_analyse= len(names)\n",
    "\n",
    "\n",
    "\n",
    "res_matrices_2=[]\n",
    "part_matrices_2=[]\n",
    "H_gc_2=[]\n",
    "\n",
    "residuals_2=[]\n",
    "soft_clus_assignment_2=[]\n",
    "gc_masks_2=[]\n",
    "\n",
    "SRC_corrected=[]\n",
    "DST_corrected=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(number_of_data_to_analyse):\n",
    "    data=mat_data[names[i]]\n",
    "    \n",
    "    img1, img2 = data[\"img1\"], data[\"img2\"]\n",
    "\n",
    "    outliers, models = vi.group_models(data)[\"outliers\"], vi.group_models(data)[\"models\"]\n",
    "    \n",
    "    points=extract_points(models,data)\n",
    "    \n",
    "    tot_src=points[1]\n",
    "    \n",
    "    tot_dst=points[2]\n",
    "    \n",
    "    points=points[0]\n",
    "    \n",
    "    H_models=[]\n",
    "    \n",
    "    H_res_models=[]\n",
    "    \n",
    "    gc_masks_i=[]\n",
    "    \n",
    "    num_of_inliers = np.sum(data[\"label\"] != 0)\n",
    "    print(num_of_inliers)\n",
    "    \n",
    "    residual_matrix = np.zeros((num_of_inliers, len(models)))\n",
    "    \n",
    "    SRC_corrected_img_i=[]\n",
    "    DST_corrected_img_i=[]\n",
    "\n",
    "    \n",
    "    for m in range(len(models)):\n",
    "        \n",
    "\n",
    "        src = tot_src[np.where(soft_clus_assignment[i][:,m]==1)] #points[\"src_points\"][m][np.where(gc_masks[i][m]==1)]\n",
    "        dst = tot_dst[np.where(soft_clus_assignment[i][:,m]==1)] #points[\"dst_points\"][m][np.where(gc_masks[i][m]==1)]\n",
    "        \n",
    "        src_1=points[\"src_points\"][m]\n",
    "        dst_1=points[\"dst_points\"][m]\n",
    "        \n",
    "        H,mask_i=verify_pygcransac_H(src,dst,img1,img2,threshold=thresholds[i][m],verbose=False)\n",
    "        \n",
    "        H_models.append(H)\n",
    "        \n",
    "        H_residual= residual_H1_wrt_H2(src,H,H_lmeds[i][m])\n",
    "        \n",
    "        H_res_models.append(H_residual)\n",
    "        \n",
    "        gc_masks_i.append(mask_i)\n",
    "        \n",
    "        plt.figure()\n",
    "\n",
    "        draw_matches(img1, img2, src_1, dst_1, matchColor=(255,0,0), mask=1 - gc_masks[i][m], H=H,show_correct=True)\n",
    "        plt.show()\n",
    "        \n",
    "        residual_matrix[:, m] = compute_residual(tot_src, tot_dst, H)\n",
    "        SRC_corrected_img_i.append(src)\n",
    "        \n",
    "        DST_corrected_img_i.append(projectiveTransform(src,H).reshape(src.shape[0],2))\n",
    "        \n",
    "    SRC_corrected.append(SRC_corrected_img_i)\n",
    "    DST_corrected.append(DST_corrected_img_i)\n",
    "        \n",
    "    res_matrices_2.append(residual_matrix)\n",
    "    \n",
    "    gc_masks_2.append(gc_masks_i)\n",
    "        \n",
    "    residuals_2.append(H_res_models)\n",
    "        \n",
    "    H_gc_2.append(H_models)\n",
    "    \n",
    "    \n",
    "    #res=build_residual_matrix(data, plot=True, verbose=False, method=\"gc-ransac\" , threshold=thresholds[i])\n",
    "    \n",
    "    part_mat=build_partition_matrix(residual_matrix)\n",
    "    labl=data[\"label\"]\n",
    "    mod_lab=np.where(labl>0)\n",
    "    labl=labl[mod_lab].reshape(-1,1)\n",
    "    labl=labl/np.max(labl)\n",
    "    labl=np.abs(np.max(part_mat)-labl*np.max(part_mat))\n",
    "    \n",
    "    res_matrices_2.append(residual_matrix)\n",
    "    part_matrices_2.append(part_mat)\n",
    "    plot_residual_matrix(part_mat,labl,title=\"a\")\n",
    "    \n",
    "    soft_clustering=soft_clustering_assignment(residual_matrix,thresholds[i])\n",
    "    soft_clus_assignment_2.append(soft_clustering)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffebbd92",
   "metadata": {},
   "source": [
    "#### Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input parameter, how many files to analyse\n",
    "number_of_data_to_analyse=len(names)\n",
    "\n",
    "\n",
    "for i in range(number_of_data_to_analyse):\n",
    "    data=mat_data[names[i]]\n",
    "    \n",
    "    outliers, models = vi.group_models(data)[\"outliers\"], vi.group_models(data)[\"models\"]\n",
    "\n",
    "    \n",
    "    img1, img2 = data[\"img1\"], data[\"img2\"]\n",
    "    \n",
    "    for m in range(len(models)):\n",
    "        src = SRC_corrected[i][m]\n",
    "        dst = DST_corrected[i][m]\n",
    "        \n",
    "        draw_matches(img1, img2, src, dst, title=\"Corrections\",matchColor=(255, 255, 0), singlePointColor=None, flags=2,\n",
    "                 mask=None, H=None,show_correct=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
